{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1514093,"sourceType":"datasetVersion","datasetId":892049},{"sourceId":2461165,"sourceType":"datasetVersion","datasetId":1489753}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sahilpawar9192/building-detections?scriptVersionId=259804225\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# --- Building Dataset Path Verification ---\nimport os\nimport glob\n\n# This is the likely base path, but we will confirm.\nBASE_PATH = '/kaggle/input/massachusetts-buildings-dataset/massachusetts-buildings-dataset'\n\nprint(f\"--- Searching for training data in: {BASE_PATH} ---\")\n\ntry:\n    # This dataset uses 'sat' for satellite images and 'map' for masks\n    train_sat_path = os.path.join(BASE_PATH, 'train', 'sat')\n    train_map_path = os.path.join(BASE_PATH, 'train', 'map')\n\n    train_images = glob.glob(os.path.join(train_sat_path, '*.tif'))\n    train_masks = glob.glob(os.path.join(train_map_path, '*.tif'))\n\n    if train_images and train_masks:\n        print(f\"✅ Success! Found {len(train_images)} training images and {len(train_masks)} masks.\")\n        print(f\"   -> Image path looks correct: {train_sat_path}\")\n        print(f\"   -> Mask path looks correct: {train_map_path}\")\n    else:\n        print(\"\\n❌ Could not find training files. Let's explore the directory.\")\n        print(\"Contents of base path:\")\n        for item in os.listdir(BASE_PATH):\n            print(f\" -> {item}\")\n\nexcept FileNotFoundError:\n    print(f\"\\n❌ FATAL ERROR: The base path '{BASE_PATH}' is incorrect.\")\n    print(\"Please find the correct path in the right-hand 'Data' panel and update the BASE_PATH variable.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:18:10.670864Z","iopub.execute_input":"2025-09-03T05:18:10.671402Z","iopub.status.idle":"2025-09-03T05:18:10.677497Z","shell.execute_reply.started":"2025-09-03T05:18:10.671378Z","shell.execute_reply":"2025-09-03T05:18:10.676722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import mobilenet_v2\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:28:49.406267Z","iopub.execute_input":"2025-09-03T05:28:49.406976Z","iopub.status.idle":"2025-09-03T05:28:49.410964Z","shell.execute_reply.started":"2025-09-03T05:28:49.406952Z","shell.execute_reply":"2025-09-03T05:28:49.410152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Diagnostic Script for Massachusetts Buildings Dataset ---\nimport os\nimport glob\n\n# The base path we are trying to use\nBASE_PATH = '/kaggle/input/d/balraj98/massachusetts-buildings-dataset/png'\n\nprint(f\"--- Running Diagnostics on Base Path: {BASE_PATH} ---\")\n\n# The specific sub-folders we need to access\ntrain_img_dir = os.path.join(BASE_PATH, 'train')\ntrain_mask_dir = os.path.join(BASE_PATH, 'train_labels')\n\nprint(f\"\\nAttempting to read images from: {train_img_dir}\")\nprint(f\"Attempting to read masks from: {train_mask_dir}\")\n\ntry:\n    # Use glob to find all .png files in these directories\n    train_images = glob.glob(os.path.join(train_img_dir, '*.png'))\n    train_masks = glob.glob(os.path.join(train_mask_dir, '*.png'))\n\n    print(\"-\" * 20)\n    print(f\"Found {len(train_images)} training images.\")\n    print(f\"Found {len(train_masks)} training masks.\")\n    print(\"-\" * 20)\n\n    if not train_images or not train_masks:\n        print(\"\\n❌ CONCLUSION: No files found. This means the path is incorrect.\")\n        print(f\"Let's check the actual contents of '{BASE_PATH}':\")\n        \n        # List what's really inside the 'png' folder\n        contents = os.listdir(BASE_PATH)\n        for item in contents:\n            print(f\" -> {item}\")\n        print(\"\\nPlease compare the names above ('train', 'train_labels') with what the script is using.\")\n\n    else:\n        print(\"\\n✅ SUCCESS: Files were found! The paths are correct.\")\n\nexcept FileNotFoundError:\n    print(f\"\\n❌ FATAL ERROR: The base path '{BASE_PATH}' does not exist.\")\n    print(\"Please check the 'Data' panel on the right to confirm the dataset's main folder name.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:32:57.627729Z","iopub.execute_input":"2025-09-03T05:32:57.628476Z","iopub.status.idle":"2025-09-03T05:32:57.639144Z","shell.execute_reply.started":"2025-09-03T05:32:57.628443Z","shell.execute_reply":"2025-09-03T05:32:57.638413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration with FINAL CORRECTED PATH ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # The correct path you discovered\n    BASE_PATH = '/kaggle/input/d/balraj98/massachusetts-buildings-dataset/png'\n    \n    IMG_HEIGHT = 256\n    IMG_WIDTH = 256\n    BATCH_SIZE = 16\n    EPOCHS = 40\n    LEARNING_RATE = 1e-4\n    MODEL_SAVE_PATH = \"/kaggle/working/building_segmentation_model.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:34:18.430288Z","iopub.execute_input":"2025-09-03T05:34:18.430563Z","iopub.status.idle":"2025-09-03T05:34:18.434922Z","shell.execute_reply.started":"2025-09-03T05:34:18.430542Z","shell.execute_reply":"2025-09-03T05:34:18.434151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CORRECTED Dataset Class ---\nimport cv2 # Make sure cv2 is imported\n\nclass BuildingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n        \n        # Load image (this part was fine)\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        \n        # --- FIX: Use OpenCV for robust mask loading and binarization ---\n        # 1. Load the mask directly in grayscale\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        # 2. Explicitly threshold the mask: any pixel value above 0 becomes 255\n        _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n        # 3. Normalize to the required 0.0 or 1.0 float format\n        mask = mask.astype(np.float32) / 255.0\n        \n        # Apply augmentations\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"]\n            \n        return image, mask.unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:48:22.113323Z","iopub.execute_input":"2025-09-03T05:48:22.114239Z","iopub.status.idle":"2025-09-03T05:48:22.120987Z","shell.execute_reply.started":"2025-09-03T05:48:22.114203Z","shell.execute_reply":"2025-09-03T05:48:22.120314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Reusing our proven MobileNetV2-U-Net Architecture ---\nclass MobileNetV2_UNet(nn.Module):\n    def __init__(self):\n        super(MobileNetV2_UNet, self).__init__()\n        self.encoder = mobilenet_v2(weights='IMAGENET1K_V1').features\n        self.skip1 = self.encoder[:2]; self.skip2 = self.encoder[2:4]\n        self.skip3 = self.encoder[4:7]; self.skip4 = self.encoder[7:14]\n        self.bridge = self.encoder[14:18]\n        self.up1 = nn.ConvTranspose2d(320, 96, 2, 2)\n        self.conv1 = nn.Sequential(nn.Conv2d(192, 96, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up2 = nn.ConvTranspose2d(96, 32, 2, 2)\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up3 = nn.ConvTranspose2d(32, 24, 2, 2)\n        self.conv3 = nn.Sequential(nn.Conv2d(48, 24, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up4 = nn.ConvTranspose2d(24, 16, 2, 2)\n        self.conv4 = nn.Sequential(nn.Conv2d(32, 16, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.final_up = nn.ConvTranspose2d(16, 16, 2, 2)\n        self.final_conv = nn.Conv2d(16, 1, 1)\n    def forward(self, x):\n        s1 = self.skip1(x); s2 = self.skip2(s1); s3 = self.skip3(s2); s4 = self.skip4(s3)\n        bridge = self.bridge(s4)\n        x = self.up1(bridge); x = torch.cat([x, s4], 1); x = self.conv1(x)\n        x = self.up2(x); x = torch.cat([x, s3], 1); x = self.conv2(x)\n        x = self.up3(x); x = torch.cat([x, s2], 1); x = self.conv3(x)\n        x = self.up4(x); x = torch.cat([x, s1], 1); x = self.conv4(x)\n        x = self.final_up(x)\n        return self.final_conv(x)\n\n# --- Advanced Loss Function for Best Accuracy ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n    def forward(self, inputs, targets, smooth=1):\n        inputs = torch.sigmoid(inputs)       \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = nn.functional.binary_cross_entropy(inputs, targets, reduction='mean')\n        return BCE + dice_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:48:26.478627Z","iopub.execute_input":"2025-09-03T05:48:26.479377Z","iopub.status.idle":"2025-09-03T05:48:26.489544Z","shell.execute_reply.started":"2025-09-03T05:48:26.479353Z","shell.execute_reply":"2025-09-03T05:48:26.488623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Training and Validation Functions ---\ndef train_fn(loader, model, optimizer, loss_fn):\n    loop = tqdm(loader, leave=True)\n    for data, targets in loop:\n        data, targets = data.to(Config.DEVICE), targets.to(Config.DEVICE)\n        predictions = model(data)\n        loss = loss_fn(predictions, targets)\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n        loop.set_postfix(loss=loss.item())\n\ndef eval_fn(loader, model, device):\n    dice_score = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n    model.train()\n    return dice_score / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:48:30.629066Z","iopub.execute_input":"2025-09-03T05:48:30.629358Z","iopub.status.idle":"2025-09-03T05:48:30.635408Z","shell.execute_reply.started":"2025-09-03T05:48:30.62934Z","shell.execute_reply":"2025-09-03T05:48:30.634428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\n\n# Define paths using the verified structure\ntrain_img_dir = os.path.join(Config.BASE_PATH, 'train')\ntrain_mask_dir = os.path.join(Config.BASE_PATH, 'train_labels')\nval_img_dir = os.path.join(Config.BASE_PATH, 'val')\nval_mask_dir = os.path.join(Config.BASE_PATH, 'val_labels')\n\n# Augmentations\ntrain_transform = A.Compose([\n    A.Resize(height=Config.IMG_HEIGHT, width=Config.IMG_WIDTH), A.Rotate(limit=35),\n    A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.1), A.ColorJitter(p=0.2),\n    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n    ToTensorV2(),\n])\nval_transform = A.Compose([\n    A.Resize(height=Config.IMG_HEIGHT, width=Config.IMG_WIDTH),\n    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n    ToTensorV2(),\n])\n\n# Create Datasets and DataLoaders\ntrain_dataset = BuildingDataset(train_img_dir, train_mask_dir, transform=train_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\nval_dataset = BuildingDataset(val_img_dir, val_mask_dir, transform=val_transform)\nval_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n\n# Initialize Model, Loss, Optimizer, and Scheduler\nmodel = MobileNetV2_UNet().to(Config.DEVICE)\nloss_fn = DiceBCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.1, verbose=True)\n\n# --- Start Training ---\nprint(\"--- Starting Building Segmentation Model Training ---\")\nbest_dice_score = -1.0\nfor epoch in range(Config.EPOCHS):\n    print(f\"Epoch {epoch+1}/{Config.EPOCHS}\")\n    train_fn(train_loader, model, optimizer, loss_fn)\n    \n    # Check validation performance\n    dice_score = eval_fn(val_loader, model, Config.DEVICE)\n    print(f\"Validation Dice Score: {dice_score:.4f}\")\n    \n    # Update learning rate\n    scheduler.step(dice_score)\n    \n    # Save the model only if it's the best one so far\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)\n        print(f\"✅ New best model saved with Dice Score: {best_dice_score:.4f}\")\n\nprint(\"\\n--- Training Complete ---\")\nprint(f\"🏆 Final best model saved to {Config.MODEL_SAVE_PATH} with a Dice Score of {best_dice_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:48:56.190556Z","iopub.execute_input":"2025-09-03T05:48:56.19113Z","iopub.status.idle":"2025-09-03T05:54:35.4979Z","shell.execute_reply.started":"2025-09-03T05:48:56.191106Z","shell.execute_reply":"2025-09-03T05:54:35.497104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# --- Diagnostic Script to Visualize a Training Batch ---\n\n# We need the BuildingDataset class to be defined.\n# If it's not in a cell above this one, you'll need to copy it here.\n\nprint(\"--- Preparing to visualize one batch of data ---\")\n\n# We use the validation transform because we want to see the clean data, not augmented.\nval_transform = A.Compose([\n    A.Resize(height=Config.IMG_HEIGHT, width=Config.IMG_WIDTH),\n    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n    ToTensorV2(),\n])\n\n# Use the validation data for a clean look\nval_img_dir = os.path.join(Config.BASE_PATH, 'val')\nval_mask_dir = os.path.join(Config.BASE_PATH, 'val_labels')\nval_dataset = BuildingDataset(val_img_dir, val_mask_dir, transform=val_transform)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True) # Get a small batch of 4\n\n# --- Fetch and Plot One Batch ---\nprint(\"Fetching one batch from the DataLoader...\")\ntry:\n    images, masks = next(iter(val_loader))\n\n    fig, axes = plt.subplots(4, 2, figsize=(10, 20))\n    fig.suptitle(\"Data Batch Visualization (Image vs. Mask)\", fontsize=16)\n\n    for i in range(4):\n        # De-normalize and prepare image for plotting\n        image = images[i].permute(1, 2, 0).numpy()\n        image = (image * 255.0).astype(np.uint8)\n        \n        # Prepare mask for plotting\n        mask = masks[i].squeeze().numpy()\n\n        axes[i, 0].imshow(image)\n        axes[i, 0].set_title(f\"Image {i+1}\")\n        axes[i, 0].axis(\"off\")\n        \n        axes[i, 1].imshow(mask, cmap='gray')\n        axes[i, 1].set_title(f\"Mask {i+1}\")\n        axes[i, 1].axis(\"off\")\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\nexcept Exception as e:\n    print(f\"\\n❌ An error occurred while trying to fetch a batch: {e}\")\n    print(\"This likely confirms a problem in the BuildingDataset class.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:43:19.984459Z","iopub.execute_input":"2025-09-03T05:43:19.985418Z","iopub.status.idle":"2025-09-03T05:43:21.614831Z","shell.execute_reply.started":"2025-09-03T05:43:19.985386Z","shell.execute_reply":"2025-09-03T05:43:21.614032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import mobilenet_v2\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport cv2\n\n# --- Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/d/balraj98/massachusetts-buildings-dataset/png'\n    \n    IMG_HEIGHT = 256\n    IMG_WIDTH = 256\n    BATCH_SIZE = 16\n    \n    # --- TWO-STAGE TRAINING PARAMS ---\n    # Stage 1: Train the decoder only\n    STAGE_1_EPOCHS = 15\n    STAGE_1_LR = 1e-3\n    # Stage 2: Fine-tune the whole model\n    STAGE_2_EPOCHS = 25\n    STAGE_2_LR = 1e-5\n    \n    MODEL_SAVE_PATH = \"/kaggle/working/building_segmentation_model_final.pt\"\n\n# --- Corrected Dataset Class ---\nclass BuildingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n        self.transform = transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n        mask = mask.astype(np.float32) / 255.0\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- Model Architecture (Unchanged) ---\nclass MobileNetV2_UNet(nn.Module):\n    def __init__(self):\n        super(MobileNetV2_UNet, self).__init__()\n        self.encoder = mobilenet_v2(weights='IMAGENET1K_V1').features\n        self.skip1 = self.encoder[:2]; self.skip2 = self.encoder[2:4]\n        self.skip3 = self.encoder[4:7]; self.skip4 = self.encoder[7:14]\n        self.bridge = self.encoder[14:18]\n        self.up1 = nn.ConvTranspose2d(320, 96, 2, 2)\n        self.conv1 = nn.Sequential(nn.Conv2d(192, 96, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up2 = nn.ConvTranspose2d(96, 32, 2, 2)\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up3 = nn.ConvTranspose2d(32, 24, 2, 2)\n        self.conv3 = nn.Sequential(nn.Conv2d(48, 24, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.up4 = nn.ConvTranspose2d(24, 16, 2, 2)\n        self.conv4 = nn.Sequential(nn.Conv2d(32, 16, 3, 1, 1, bias=False), nn.ReLU(inplace=True))\n        self.final_up = nn.ConvTranspose2d(16, 16, 2, 2)\n        self.final_conv = nn.Conv2d(16, 1, 1)\n    def forward(self, x):\n        s1 = self.skip1(x); s2 = self.skip2(s1); s3 = self.skip3(s2); s4 = self.skip4(s3)\n        bridge = self.bridge(s4)\n        x = self.up1(bridge); x = torch.cat([x, s4], 1); x = self.conv1(x)\n        x = self.up2(x); x = torch.cat([x, s3], 1); x = self.conv2(x)\n        x = self.up3(x); x = torch.cat([x, s2], 1); x = self.conv3(x)\n        x = self.up4(x); x = torch.cat([x, s1], 1); x = self.conv4(x)\n        x = self.final_up(x)\n        return self.final_conv(x)\n\n# --- Loss Function and Training/Eval Functions (Unchanged) ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self): super(DiceBCELoss, self).__init__()\n    def forward(self, inputs, targets, smooth=1):\n        inputs = torch.sigmoid(inputs)\n        inputs, targets = inputs.view(-1), targets.view(-1)\n        intersection = (inputs * targets).sum()\n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n        BCE = nn.functional.binary_cross_entropy(inputs, targets, reduction='mean')\n        return BCE + dice_loss\n\ndef train_fn(loader, model, optimizer, loss_fn):\n    loop = tqdm(loader, leave=True)\n    for data, targets in loop:\n        data, targets = data.to(Config.DEVICE), targets.to(Config.DEVICE)\n        predictions = model(data); loss = loss_fn(predictions, targets)\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n        loop.set_postfix(loss=loss.item())\n\ndef eval_fn(loader, model, device):\n    dice_score = 0; model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            preds = torch.sigmoid(model(x)); preds = (preds > 0.5).float()\n            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n    model.train(); return dice_score / len(loader)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\ntrain_img_dir = os.path.join(Config.BASE_PATH, 'train'); train_mask_dir = os.path.join(Config.BASE_PATH, 'train_labels')\nval_img_dir = os.path.join(Config.BASE_PATH, 'val'); val_mask_dir = os.path.join(Config.BASE_PATH, 'val_labels')\ntrain_transform = A.Compose([A.Resize(Config.IMG_HEIGHT, Config.IMG_WIDTH), A.Rotate(limit=35), A.HorizontalFlip(), A.VerticalFlip(), A.Normalize(mean=0, std=1), ToTensorV2()])\nval_transform = A.Compose([A.Resize(Config.IMG_HEIGHT, Config.IMG_WIDTH), A.Normalize(mean=0, std=1), ToTensorV2()])\ntrain_dataset = BuildingDataset(train_img_dir, train_mask_dir, transform=train_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\nval_dataset = BuildingDataset(val_img_dir, val_mask_dir, transform=val_transform)\nval_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\nmodel = MobileNetV2_UNet().to(Config.DEVICE); loss_fn = DiceBCELoss()\n\n# --- STAGE 1: TRAIN DECODER ---\nprint(\"--- Stage 1: Training the Decoder ---\")\n# Freeze the encoder layers\nfor param in model.encoder.parameters():\n    param.requires_grad = False\noptimizer = torch.optim.Adam(model.parameters(), lr=Config.STAGE_1_LR)\nfor epoch in range(Config.STAGE_1_EPOCHS):\n    print(f\"Stage 1 - Epoch {epoch+1}/{Config.STAGE_1_EPOCHS}\")\n    train_fn(train_loader, model, optimizer, loss_fn)\n\n# --- STAGE 2: FINE-TUNE ENTIRE MODEL ---\nprint(\"\\n--- Stage 2: Fine-Tuning the Entire Model ---\")\n# Unfreeze all layers\nfor param in model.encoder.parameters():\n    param.requires_grad = True\noptimizer = torch.optim.Adam(model.parameters(), lr=Config.STAGE_2_LR)\nscheduler = ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.1, verbose=True)\nbest_dice_score = -1.0\nfor epoch in range(Config.STAGE_2_EPOCHS):\n    print(f\"Stage 2 - Epoch {epoch+1}/{Config.STAGE_2_EPOCHS}\")\n    train_fn(train_loader, model, optimizer, loss_fn)\n    dice_score = eval_fn(val_loader, model, Config.DEVICE)\n    print(f\"Validation Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)\n        print(f\"✅ New best model saved with Dice Score: {best_dice_score:.4f}\")\n\nprint(\"\\n--- Training Complete ---\")\nprint(f\"🏆 Final best model saved to {Config.MODEL_SAVE_PATH} with a Dice Score of {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T05:58:00.249803Z","iopub.execute_input":"2025-09-03T05:58:00.250657Z","iopub.status.idle":"2025-09-03T06:03:28.829842Z","shell.execute_reply.started":"2025-09-03T05:58:00.250627Z","shell.execute_reply":"2025-09-03T06:03:28.828759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport cv2\n\n# --- Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/d/balraj98/massachusetts-buildings-dataset/png'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 16\n    STAGE_1_EPOCHS = 10 # Fewer epochs needed for the powerful ResNet\n    STAGE_1_LR = 1e-3\n    STAGE_2_EPOCHS = 30\n    STAGE_2_LR = 1e-5\n    MODEL_SAVE_PATH = \"/kaggle/working/resnet34_unet_buildings.pt\"\n\n# --- Corrected Dataset Class (Unchanged) ---\nclass BuildingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n        self.transform = transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n        mask = mask.astype(np.float32) / 255.0\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- NEW: ResNet34 U-Net Architecture ---\nclass ResNet_UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        # Encoder\n        base_model = resnet34(weights='IMAGENET1K_V1')\n        self.base_layers = list(base_model.children())\n        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x/2, y/2)\n        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x/4, y/4)\n        self.layer2 = self.base_layers[5] # size=(N, 128, x/8, y/8)\n        self.layer3 = self.base_layers[6] # size=(N, 256, x/16, y/16)\n        self.layer4 = self.base_layers[7] # size=(N, 512, x/32, y/32)\n\n        # Decoder\n        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=True))\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.conv2 = nn.Sequential(nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(inplace=True))\n        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.conv3 = nn.Sequential(nn.Conv2d(128, 64, 3, 1, 1), nn.ReLU(inplace=True))\n        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n        self.conv4 = nn.Sequential(nn.Conv2d(128, 64, 3, 1, 1), nn.ReLU(inplace=True))\n        \n        self.final_up = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        l0 = self.layer0(x)\n        l1 = self.layer1(l0)\n        l2 = self.layer2(l1)\n        l3 = self.layer3(l2)\n        l4 = self.layer4(l3)\n        \n        x = self.up1(l4); x = torch.cat([x, l3], dim=1); x = self.conv1(x)\n        x = self.up2(x); x = torch.cat([x, l2], dim=1); x = self.conv2(x)\n        x = self.up3(x); x = torch.cat([x, l1], dim=1); x = self.conv3(x)\n        x = self.up4(x); x = torch.cat([x, l0], dim=1); x = self.conv4(x)\n        \n        x = self.final_up(x)\n        return self.final_conv(x)\n\n# --- Loss Function and Training/Eval Functions (Unchanged) ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, i, t, s=1): i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef train_fn(ldr, mdl, opt, l_fn):\n    loop=tqdm(ldr,leave=True)\n    for d, t in loop: d,t=d.to(Config.DEVICE),t.to(Config.DEVICE);p=mdl(d);l=l_fn(p,t);opt.zero_grad();l.backward();opt.step();loop.set_postfix(loss=l.item())\ndef eval_fn(ldr, mdl, dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr: x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\ntrain_img_dir=os.path.join(Config.BASE_PATH,'train');train_mask_dir=os.path.join(Config.BASE_PATH,'train_labels')\nval_img_dir=os.path.join(Config.BASE_PATH,'val');val_mask_dir=os.path.join(Config.BASE_PATH,'val_labels')\ntrain_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Rotate(35),A.HorizontalFlip(),A.VerticalFlip(p=0.1),A.Normalize(0,1),ToTensorV2()])\nval_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Normalize(0,1),ToTensorV2()])\ntrain_dataset=BuildingDataset(train_img_dir,train_mask_dir,transform=train_transform);val_dataset=BuildingDataset(val_img_dir,val_mask_dir,transform=val_transform)\ntrain_loader=DataLoader(train_dataset,Config.BATCH_SIZE,shuffle=True,num_workers=2);val_loader=DataLoader(val_dataset,Config.BATCH_SIZE,shuffle=False,num_workers=2)\nmodel=ResNet_UNet().to(Config.DEVICE);loss_fn=DiceBCELoss()\n\n# --- STAGE 1: TRAIN DECODER ---\nprint(\"--- Stage 1: Training the Decoder ---\")\nfor param in model.base_layers: param.requires_grad=False\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_1_LR)\nfor epoch in range(Config.STAGE_1_EPOCHS): print(f\"S1 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n\n# --- STAGE 2: FINE-TUNE ENTIRE MODEL ---\nprint(\"\\n--- Stage 2: Fine-Tuning the Entire Model ---\")\nfor param in model.base_layers: param.requires_grad=True\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_2_LR)\nscheduler=ReduceLROnPlateau(optimizer,'max',patience=3,factor=0.1,verbose=True)\nbest_dice_score=-1.0\nfor epoch in range(Config.STAGE_2_EPOCHS):\n    print(f\"S2 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n    dice_score=eval_fn(val_loader,model,Config.DEVICE)\n    print(f\"Val Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score > best_dice_score: best_dice_score=dice_score;torch.save(model.state_dict(),Config.MODEL_SAVE_PATH);print(f\"✅ New best model saved: {best_dice_score:.4f}\")\n\nprint(f\"\\n🏆 Final best model saved: {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:46:53.477077Z","iopub.execute_input":"2025-09-03T07:46:53.477338Z","iopub.status.idle":"2025-09-03T07:52:55.772442Z","shell.execute_reply.started":"2025-09-03T07:46:53.477317Z","shell.execute_reply":"2025-09-03T07:52:55.771514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport cv2\n\n# --- Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/d/balraj98/massachusetts-buildings-dataset/png'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 8 # HRNet uses more memory, so we reduce the batch size\n    EPOCHS = 40\n    LEARNING_RATE = 1e-4\n    MODEL_SAVE_PATH = \"/kaggle/working/hrnet_buildings_final.pt\"\n\n# --- Dataset Class (Unchanged) ---\nclass BuildingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n        self.transform = transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n        mask = mask.astype(np.float32) / 255.0\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- NEW: HRNet Architecture ---\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) if stride != 1 or inplanes != planes else None\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n        out = self.conv2(out); out = self.bn2(out)\n        if self.downsample is not None: identity = self.downsample(x)\n        out += identity; out = self.relu(out)\n        return out\n\nclass HRNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super(HRNet, self).__init__()\n        # Stem\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Stage 1\n        self.layer1 = self._make_layer(BasicBlock, 64, 64, 4)\n        \n        # Transition 1\n        self.transition1_1 = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1, bias=False), nn.BatchNorm2d(32), nn.ReLU(True))\n        self.transition1_2 = nn.Sequential(nn.Conv2d(64, 64, 3, 2, 1, bias=False), nn.BatchNorm2d(64), nn.ReLU(True))\n\n        # Stage 2\n        self.stage2_1 = nn.Sequential(BasicBlock(32, 32), BasicBlock(32, 32))\n        self.stage2_2 = nn.Sequential(BasicBlock(64, 64), BasicBlock(64, 64))\n\n        # Fusion 1\n        self.fuse1_1 = nn.Sequential(nn.Conv2d(64, 32, 1, 1, 0, bias=False), nn.BatchNorm2d(32), nn.ReLU(True), nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))\n        \n        # Final Layers\n        self.final_conv = nn.Conv2d(32, 32, kernel_size=1)\n        self.out = nn.Conv2d(32, n_classes, kernel_size=1)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        layers = [block(inplanes, planes, stride)]\n        for _ in range(1, blocks): layers.append(block(planes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x); x = self.bn1(x); x = self.relu(x)\n        x = self.conv2(x); x = self.bn2(x); x = self.relu(x)\n        x = self.layer1(x)\n        \n        x1 = self.transition1_1(x)\n        x2 = self.transition1_2(x)\n\n        x1 = self.stage2_1(x1)\n        x2 = self.stage2_2(x2)\n\n        x2_fused = self.fuse1_1(x2)\n        x = x1 + x2_fused\n        \n        x = nn.functional.interpolate(x, scale_factor=4, mode='bilinear', align_corners=True)\n        x = self.final_conv(x)\n        return self.out(x)\n\n# --- Loss Function and Training/Eval Functions (Unchanged) ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, i, t, s=1): i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef train_fn(ldr, mdl, opt, l_fn):\n    loop=tqdm(ldr,leave=True)\n    for d, t in loop: d,t=d.to(Config.DEVICE),t.to(Config.DEVICE);p=mdl(d);l=l_fn(p,t);opt.zero_grad();l.backward();opt.step();loop.set_postfix(loss=l.item())\ndef eval_fn(ldr, mdl, dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr: x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\ntrain_img_dir=os.path.join(Config.BASE_PATH,'train');train_mask_dir=os.path.join(Config.BASE_PATH,'train_labels')\nval_img_dir=os.path.join(Config.BASE_PATH,'val');val_mask_dir=os.path.join(Config.BASE_PATH,'val_labels')\ntrain_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Rotate(35),A.HorizontalFlip(),A.VerticalFlip(p=0.1),A.Normalize(0,1),ToTensorV2()])\nval_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Normalize(0,1),ToTensorV2()])\ntrain_dataset=BuildingDataset(train_img_dir,train_mask_dir,transform=train_transform);val_dataset=BuildingDataset(val_img_dir,val_mask_dir,transform=val_transform)\ntrain_loader=DataLoader(train_dataset,Config.BATCH_SIZE,shuffle=True,num_workers=2);val_loader=DataLoader(val_dataset,Config.BATCH_SIZE,shuffle=False,num_workers=2)\nmodel=HRNet().to(Config.DEVICE);loss_fn=DiceBCELoss()\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.LEARNING_RATE)\nscheduler=ReduceLROnPlateau(optimizer,'max',patience=3,factor=0.1,verbose=True)\nbest_dice_score=-1.0\nfor epoch in range(Config.EPOCHS):\n    print(f\"Epoch {epoch+1}/{Config.EPOCHS}\");train_fn(train_loader,model,optimizer,loss_fn)\n    dice_score=eval_fn(val_loader,model,Config.DEVICE)\n    print(f\"Val Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score > best_dice_score: best_dice_score=dice_score;torch.save(model.state_dict(),Config.MODEL_SAVE_PATH);print(f\"✅ New best model saved: {best_dice_score:.4f}\")\n\nprint(f\"\\n🏆 Final best model saved: {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:03:07.477134Z","iopub.execute_input":"2025-09-03T08:03:07.478135Z","iopub.status.idle":"2025-09-03T08:08:46.1844Z","shell.execute_reply.started":"2025-09-03T08:03:07.478101Z","shell.execute_reply":"2025-09-03T08:08:46.183383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n# --- Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/inria-aerial-image-labeling-dataset/AerialImageDataset'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 16\n    STAGE_1_EPOCHS = 10\n    STAGE_1_LR = 1e-3\n    STAGE_2_EPOCHS = 30\n    STAGE_2_LR = 1e-5\n    MODEL_SAVE_PATH = \"/kaggle/working/inria_resnet34_unet_final.pt\"\n\n# --- Dataset Class for Inria ---\nclass InriaDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        mask = (mask > 0).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- ResNet34 U-Net Architecture ---\nclass ResNet_UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model=resnet34(weights='IMAGENET1K_V1');self.base_layers=list(base_model.children())\n        self.layer0=nn.Sequential(*self.base_layers[:3]);self.layer1=nn.Sequential(*self.base_layers[3:5])\n        self.layer2,self.layer3,self.layer4=self.base_layers[5],self.base_layers[6],self.base_layers[7]\n        self.up1=nn.ConvTranspose2d(512,256,2,2);self.conv1=nn.Sequential(nn.Conv2d(512,256,3,1,1),nn.ReLU(True))\n        self.up2=nn.ConvTranspose2d(256,128,2,2);self.conv2=nn.Sequential(nn.Conv2d(256,128,3,1,1),nn.ReLU(True))\n        self.up3=nn.ConvTranspose2d(128,64,2,2);self.conv3=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.up4=nn.ConvTranspose2d(64,64,2,2);self.conv4=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.final_up=nn.ConvTranspose2d(64,32,2,2);self.final_conv=nn.Conv2d(32,n_classes,1)\n    def forward(self,x):\n        l0=self.layer0(x);l1=self.layer1(l0);l2=self.layer2(l1);l3=self.layer3(l2);l4=self.layer4(l3)\n        u1=self.up1(l4);c1=torch.cat([u1,l3],1);e1=self.conv1(c1)\n        u2=self.up2(e1);c2=torch.cat([u2,l2],1);e2=self.conv2(c2)\n        u3=self.up3(e2);c3=torch.cat([u3,l1],1);e3=self.conv3(c3)\n        u4=self.up4(e3);c4=torch.cat([u4,l0],1);e4=self.conv4(c4)\n        final=self.final_up(e4);return self.final_conv(final)\n\n# --- Loss, Train, Eval Functions ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self):super().__init__()\n    def forward(self,i,t,s=1):i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef train_fn(ldr,mdl,opt,l_fn):\n    loop=tqdm(ldr,leave=True)\n    for d,t in loop:d,t=d.to(Config.DEVICE),t.to(Config.DEVICE);p=mdl(d);l=l_fn(p,t);opt.zero_grad();l.backward();opt.step();loop.set_postfix(loss=l.item())\ndef eval_fn(ldr,mdl,dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr:x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\n\n# --- NEW: Split the training data into train/val sets ---\ntrain_img_dir=os.path.join(Config.BASE_PATH,'train/images')\ntrain_mask_dir=os.path.join(Config.BASE_PATH,'train/gt')\nall_train_images = sorted(glob.glob(os.path.join(train_img_dir, '*.tif')))\nall_train_masks = sorted(glob.glob(os.path.join(train_mask_dir, '*.tif')))\n\n# Use sklearn to create a robust split\ntrain_images, val_images, train_masks, val_masks = train_test_split(\n    all_train_images, all_train_masks, test_size=0.2, random_state=42\n)\nprint(f\"Data split: {len(train_images)} training images, {len(val_images)} validation images.\")\n\n\ntrain_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Rotate(35),A.HorizontalFlip(),A.Normalize(0,1),ToTensorV2()])\nval_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Normalize(0,1),ToTensorV2()])\ntrain_dataset=InriaDataset(train_images,train_masks,transform=train_transform) # Use split data\nval_dataset=InriaDataset(val_images,val_masks,transform=val_transform) # Use split data\ntrain_loader=DataLoader(train_dataset,Config.BATCH_SIZE,shuffle=True,num_workers=2)\nval_loader=DataLoader(val_dataset,Config.BATCH_SIZE,shuffle=False,num_workers=2)\n\nmodel=ResNet_UNet().to(Config.DEVICE);loss_fn=DiceBCELoss()\n\n# --- STAGE 1 ---\nprint(\"--- Stage 1: Training the Decoder ---\")\nfor p in model.base_layers:p.requires_grad=False\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_1_LR)\nfor epoch in range(Config.STAGE_1_EPOCHS):print(f\"S1 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n\n# --- STAGE 2 ---\nprint(\"\\n--- Stage 2: Fine-Tuning the Entire Model ---\")\nfor p in model.base_layers:p.requires_grad=True\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_2_LR)\nscheduler=ReduceLROnPlateau(optimizer,'max',patience=3,factor=0.1,verbose=True)\nbest_dice_score=-1.0\nfor epoch in range(Config.STAGE_2_EPOCHS):\n    print(f\"S2 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n    dice_score=eval_fn(val_loader,model,Config.DEVICE)\n    print(f\"Val Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score>best_dice_score:best_dice_score=dice_score;torch.save(model.state_dict(),Config.MODEL_SAVE_PATH);print(f\"✅ New best model saved: {best_dice_score:.4f}\")\n\nprint(f\"\\n🏆 Final best model saved: {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T08:50:30.165848Z","iopub.execute_input":"2025-09-03T08:50:30.166185Z","iopub.status.idle":"2025-09-03T09:13:02.3426Z","shell.execute_reply.started":"2025-09-03T08:50:30.166154Z","shell.execute_reply":"2025-09-03T09:13:02.34151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n# --- FINAL TUNED Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/inria-aerial-image-labeling-dataset/AerialImageDataset'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 16\n    \n    # --- TUNED PARAMETERS ---\n    # We will train for much longer to allow the model to fully converge\n    STAGE_1_EPOCHS = 15\n    STAGE_1_LR = 5e-4 # Slightly lower initial LR for the decoder\n    STAGE_2_EPOCHS = 70 # Significantly more epochs for deep fine-tuning\n    STAGE_2_LR = 1e-4 # Higher fine-tuning LR to encourage more learning\n    \n    MODEL_SAVE_PATH = \"/kaggle/working/inria_resnet34_unet_final_best.pt\"\n\n# --- Dataset Class (Unchanged) ---\nclass InriaDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        mask = (mask > 0).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- ResNet34 U-Net Architecture (Unchanged) ---\nclass ResNet_UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model=resnet34(weights='IMAGENET1K_V1');self.base_layers=list(base_model.children())\n        self.layer0=nn.Sequential(*self.base_layers[:3]);self.layer1=nn.Sequential(*self.base_layers[3:5])\n        self.layer2,self.layer3,self.layer4=self.base_layers[5],self.base_layers[6],self.base_layers[7]\n        self.up1=nn.ConvTranspose2d(512,256,2,2);self.conv1=nn.Sequential(nn.Conv2d(512,256,3,1,1),nn.ReLU(True))\n        self.up2=nn.ConvTranspose2d(256,128,2,2);self.conv2=nn.Sequential(nn.Conv2d(256,128,3,1,1),nn.ReLU(True))\n        self.up3=nn.ConvTranspose2d(128,64,2,2);self.conv3=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.up4=nn.ConvTranspose2d(64,64,2,2);self.conv4=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.final_up=nn.ConvTranspose2d(64,32,2,2);self.final_conv=nn.Conv2d(32,n_classes,1)\n    def forward(self,x):\n        l0=self.layer0(x);l1=self.layer1(l0);l2=self.layer2(l1);l3=self.layer3(l2);l4=self.layer4(l3)\n        u1=self.up1(l4);c1=torch.cat([u1,l3],1);e1=self.conv1(c1)\n        u2=self.up2(e1);c2=torch.cat([u2,l2],1);e2=self.conv2(c2)\n        u3=self.up3(e2);c3=torch.cat([u3,l1],1);e3=self.conv3(c3)\n        u4=self.up4(e3);c4=torch.cat([u4,l0],1);e4=self.conv4(c4)\n        final=self.final_up(e4);return self.final_conv(final)\n\n# --- Loss, Train, Eval Functions (Unchanged) ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self):super().__init__()\n    def forward(self,i,t,s=1):i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef train_fn(ldr,mdl,opt,l_fn):\n    loop=tqdm(ldr,leave=True)\n    for d,t in loop:d,t=d.to(Config.DEVICE),t.to(Config.DEVICE);p=mdl(d);l=l_fn(p,t);opt.zero_grad();l.backward();opt.step();loop.set_postfix(loss=l.item())\ndef eval_fn(ldr,mdl,dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr:x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\n\ntrain_img_dir=os.path.join(Config.BASE_PATH,'train/images');train_mask_dir=os.path.join(Config.BASE_PATH,'train/gt')\nall_train_images=sorted(glob.glob(os.path.join(train_img_dir,'*.tif')));all_train_masks=sorted(glob.glob(os.path.join(train_mask_dir,'*.tif')))\ntrain_images,val_images,train_masks,val_masks=train_test_split(all_train_images,all_train_masks,test_size=0.2,random_state=42)\nprint(f\"Data split: {len(train_images)} training, {len(val_images)} validation.\")\n\n# --- Richer Augmentations ---\ntrain_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Rotate(35),A.HorizontalFlip(),A.VerticalFlip(p=0.1),A.ColorJitter(p=0.2),A.Normalize(0,1),ToTensorV2()])\nval_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Normalize(0,1),ToTensorV2()])\ntrain_dataset=InriaDataset(train_images,train_masks,transform=train_transform)\nval_dataset=InriaDataset(val_images,val_masks,transform=val_transform)\ntrain_loader=DataLoader(train_dataset,Config.BATCH_SIZE,shuffle=True,num_workers=2)\nval_loader=DataLoader(val_dataset,Config.BATCH_SIZE,shuffle=False,num_workers=2)\n\nmodel=ResNet_UNet().to(Config.DEVICE);loss_fn=DiceBCELoss()\n\n# --- STAGE 1 ---\nprint(\"--- Stage 1: Training the Decoder ---\")\nfor p in model.base_layers:p.requires_grad=False\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_1_LR)\nfor epoch in range(Config.STAGE_1_EPOCHS):print(f\"S1 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n\n# --- STAGE 2 ---\nprint(\"\\n--- Stage 2: Fine-Tuning the Entire Model ---\")\nfor p in model.base_layers:p.requires_grad=True\noptimizer=torch.optim.Adam(model.parameters(),lr=Config.STAGE_2_LR)\nscheduler=ReduceLROnPlateau(optimizer,'max',patience=5,factor=0.2,verbose=True) # More patient scheduler\nbest_dice_score=-1.0\nfor epoch in range(Config.STAGE_2_EPOCHS):\n    print(f\"S2 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n    dice_score=eval_fn(val_loader,model,Config.DEVICE)\n    print(f\"Val Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score>best_dice_score:best_dice_score=dice_score;torch.save(model.state_dict(),Config.MODEL_SAVE_PATH);print(f\"✅ New best model saved: {best_dice_score:.4f}\")\n\nprint(f\"\\n🏆 Final best model saved: {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T10:35:14.345042Z","iopub.execute_input":"2025-09-03T10:35:14.345603Z","iopub.status.idle":"2025-09-03T11:35:21.660944Z","shell.execute_reply.started":"2025-09-03T10:35:14.345581Z","shell.execute_reply":"2025-09-03T11:35:21.660064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n# --- FINAL TUNED Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/inria-aerial-image-labeling-dataset/AerialImageDataset'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 16\n    \n    # --- TUNED PARAMETERS FOR MAX ACCURACY ---\n    STAGE_1_EPOCHS = 15\n    STAGE_1_LR = 3e-4 \n    STAGE_2_EPOCHS = 60 # A long fine-tuning stage\n    STAGE_2_LR = 2e-5 # A very small learning rate for careful fine-tuning\n    \n    MODEL_SAVE_PATH = \"/kaggle/working/inria_resnet34_unet_BEST.pt\"\n\n# --- Dataset Class (Unchanged) ---\nclass InriaDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths, self.mask_paths, self.transform = image_paths, mask_paths, transform\n    def __len__(self): return len(self.image_paths)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        mask = (mask > 0).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\n\n# --- ResNet34 U-Net Architecture (Unchanged) ---\nclass ResNet_UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model=resnet34(weights='IMAGENET1K_V1');self.base_layers=list(base_model.children())\n        self.layer0=nn.Sequential(*self.base_layers[:3]);self.layer1=nn.Sequential(*self.base_layers[3:5])\n        self.layer2,self.layer3,self.layer4=self.base_layers[5],self.base_layers[6],self.base_layers[7]\n        self.up1=nn.ConvTranspose2d(512,256,2,2);self.conv1=nn.Sequential(nn.Conv2d(512,256,3,1,1),nn.ReLU(True))\n        self.up2=nn.ConvTranspose2d(256,128,2,2);self.conv2=nn.Sequential(nn.Conv2d(256,128,3,1,1),nn.ReLU(True))\n        self.up3=nn.ConvTranspose2d(128,64,2,2);self.conv3=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.up4=nn.ConvTranspose2d(64,64,2,2);self.conv4=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.final_up=nn.ConvTranspose2d(64,32,2,2);self.final_conv=nn.Conv2d(32,n_classes,1)\n    def forward(self,x):\n        l0=self.layer0(x);l1=self.layer1(l0);l2=self.layer2(l1);l3=self.layer3(l2);l4=self.layer4(l3)\n        u1=self.up1(l4);c1=torch.cat([u1,l3],1);e1=self.conv1(c1)\n        u2=self.up2(e1);c2=torch.cat([u2,l2],1);e2=self.conv2(c2)\n        u3=self.up3(e2);c3=torch.cat([u3,l1],1);e3=self.conv3(c3)\n        u4=self.up4(e3);c4=torch.cat([u4,l0],1);e4=self.conv4(c4)\n        final=self.final_up(e4);return self.final_conv(final)\n\n# --- Loss, Train, Eval Functions (Unchanged) ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self):super().__init__()\n    def forward(self,i,t,s=1):i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef train_fn(ldr,mdl,opt,l_fn):\n    loop=tqdm(ldr,leave=True)\n    for d,t in loop:d,t=d.to(Config.DEVICE),t.to(Config.DEVICE);p=mdl(d);l=l_fn(p,t);opt.zero_grad();l.backward();opt.step();loop.set_postfix(loss=l.item())\ndef eval_fn(ldr,mdl,dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr:x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- Main Execution ---\nprint(f\"--- Using device: {Config.DEVICE} ---\")\ntrain_img_dir=os.path.join(Config.BASE_PATH,'train/images');train_mask_dir=os.path.join(Config.BASE_PATH,'train/gt')\nall_train_images=sorted(glob.glob(os.path.join(train_img_dir,'*.tif')));all_train_masks=sorted(glob.glob(os.path.join(train_mask_dir,'*.tif')))\ntrain_images,val_images,train_masks,val_masks=train_test_split(all_train_images,all_train_masks,test_size=0.2,random_state=42)\nprint(f\"Data split: {len(train_images)} training, {len(val_images)} validation.\")\ntrain_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Rotate(35),A.HorizontalFlip(),A.Normalize(0,1),ToTensorV2()])\nval_transform=A.Compose([A.Resize(Config.IMG_HEIGHT,Config.IMG_WIDTH),A.Normalize(0,1),ToTensorV2()])\ntrain_dataset=InriaDataset(train_images,train_masks,transform=train_transform);val_dataset=InriaDataset(val_images,val_masks,transform=val_transform)\ntrain_loader=DataLoader(train_dataset,Config.BATCH_SIZE,shuffle=True,num_workers=2);val_loader=DataLoader(val_dataset,Config.BATCH_SIZE,shuffle=False,num_workers=2)\n\nmodel=ResNet_UNet().to(Config.DEVICE);loss_fn=DiceBCELoss()\n\n# --- STAGE 1 ---\nprint(\"--- Stage 1: Training the Decoder ---\")\nfor p in model.base_layers:p.requires_grad=False\noptimizer=torch.optim.AdamW(model.parameters(),lr=Config.STAGE_1_LR) # --- TUNED ---\nfor epoch in range(Config.STAGE_1_EPOCHS):print(f\"S1 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n\n# --- STAGE 2 ---\nprint(\"\\n--- Stage 2: Fine-Tuning the Entire Model ---\")\nfor p in model.base_layers:p.requires_grad=True\noptimizer=torch.optim.AdamW(model.parameters(),lr=Config.STAGE_2_LR) # --- TUNED ---\nscheduler=ReduceLROnPlateau(optimizer,'max',patience=3,factor=0.5,verbose=True) # --- TUNED ---\nbest_dice_score=-1.0\nfor epoch in range(Config.STAGE_2_EPOCHS):\n    print(f\"S2 - E{epoch+1}\");train_fn(train_loader,model,optimizer,loss_fn)\n    dice_score=eval_fn(val_loader,model,Config.DEVICE)\n    print(f\"Val Dice Score: {dice_score:.4f}\")\n    scheduler.step(dice_score)\n    if dice_score>best_dice_score:best_dice_score=dice_score;torch.save(model.state_dict(),Config.MODEL_SAVE_PATH);print(f\"✅ New best model saved: {best_dice_score:.4f}\")\n\nprint(f\"\\n🏆 Final best model saved: {best_dice_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:51:42.322551Z","iopub.execute_input":"2025-09-03T11:51:42.323432Z","iopub.status.idle":"2025-09-03T12:27:29.995676Z","shell.execute_reply.started":"2025-09-03T11:51:42.323396Z","shell.execute_reply":"2025-09-03T12:27:29.994622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\nimport optuna # The new library for hyperparameter optimization\n\n# --- Configuration ---\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    BASE_PATH = '/kaggle/input/inria-aerial-image-labeling-dataset/AerialImageDataset'\n    IMG_HEIGHT, IMG_WIDTH = 256, 256\n    BATCH_SIZE = 16 # We will keep this fixed for now\n    EPOCHS = 20 # Each trial will run for a shorter time to allow for more trials\n    MODEL_SAVE_PATH = \"/kaggle/working/inria_resnet34_unet_OPTIMIZED.pt\"\n\n# --- Dataset and Model Classes (Unchanged) ---\n# --- CORRECTED InriaDataset Class ---\nclass InriaDataset(Dataset):\n    # This __init__ line is the only change needed.\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path, mask_path = self.image_paths[idx], self.mask_paths[idx]\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n        mask = (mask > 0).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented[\"image\"], augmented[\"mask\"]\n        return image, mask.unsqueeze(0)\nclass ResNet_UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model=resnet34(weights='IMAGENET1K_V1');self.base_layers=list(base_model.children())\n        self.layer0=nn.Sequential(*self.base_layers[:3]);self.layer1=nn.Sequential(*self.base_layers[3:5])\n        self.layer2,self.layer3,self.layer4=self.base_layers[5],self.base_layers[6],self.base_layers[7]\n        self.up1=nn.ConvTranspose2d(512,256,2,2);self.conv1=nn.Sequential(nn.Conv2d(512,256,3,1,1),nn.ReLU(True))\n        self.up2=nn.ConvTranspose2d(256,128,2,2);self.conv2=nn.Sequential(nn.Conv2d(256,128,3,1,1),nn.ReLU(True))\n        self.up3=nn.ConvTranspose2d(128,64,2,2);self.conv3=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.up4=nn.ConvTranspose2d(64,64,2,2);self.conv4=nn.Sequential(nn.Conv2d(128,64,3,1,1),nn.ReLU(True))\n        self.final_up=nn.ConvTranspose2d(64,32,2,2);self.final_conv=nn.Conv2d(32,n_classes,1)\n    def forward(self,x):\n        l0=self.layer0(x);l1=self.layer1(l0);l2=self.layer2(l1);l3=self.layer3(l2);l4=self.layer4(l3)\n        u1=self.up1(l4);c1=torch.cat([u1,l3],1);e1=self.conv1(c1);u2=self.up2(e1);c2=torch.cat([u2,l2],1);e2=self.conv2(c2)\n        u3=self.up3(e2);c3=torch.cat([u3,l1],1);e3=self.conv3(c3);u4=self.up4(e3);c4=torch.cat([u4,l0],1);e4=self.conv4(c4)\n        final=self.final_up(e4);return self.final_conv(final)\nclass DiceBCELoss(nn.Module):\n    def __init__(self):super().__init__()\n    def forward(self,i,t,s=1):i=torch.sigmoid(i);i,t=i.view(-1),t.view(-1);n=(i*t).sum();l=1-(2.*n+s)/(i.sum()+t.sum()+s);b=nn.functional.binary_cross_entropy(i,t,reduction='mean');return b+l\ndef eval_fn(ldr,mdl,dev):\n    ds=0;mdl.eval()\n    with torch.no_grad():\n        for x,y in ldr:x,y=x.to(dev),y.to(dev);p=torch.sigmoid(mdl(x));p=(p>0.5).float();ds+=(2*(p*y).sum())/((p+y).sum()+1e-8)\n    mdl.train();return ds/len(ldr)\n\n# --- NEW: The Optuna Objective Function ---\n# This function defines ONE training experiment. Optuna will call this many times.\ndef objective(trial):\n    # --- 1. Suggest Hyperparameters ---\n    # Optuna will pick values for these from the ranges we define.\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"])\n    \n    # --- 2. Create Model and Optimizer with Suggested Params ---\n    model = ResNet_UNet().to(Config.DEVICE)\n    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n    loss_fn = DiceBCELoss()\n\n    # --- 3. Run the Training Loop ---\n    # We will use the same data loaders for every trial\n    for epoch in range(Config.EPOCHS):\n        # We don't need the progress bar for the automatic search\n        for data, targets in train_loader:\n            data, targets = data.to(Config.DEVICE), targets.to(Config.DEVICE)\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n        # --- 4. Report the performance to Optuna ---\n        # Optuna uses this score to decide which params are better\n        dice_score = eval_fn(val_loader, model, Config.DEVICE)\n        trial.report(dice_score, epoch)\n        \n        # Pruning: Stop unpromising trials early\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    # --- 5. Return the Final Score ---\n    # The function must return the metric we want to maximize\n    return dice_score\n\n# --- Main Execution ---\n# Prepare data once\ntrain_img_dir = os.path.join(Config.BASE_PATH, 'train/images')\ntrain_mask_dir = os.path.join(Config.BASE_PATH, 'train/gt')\nall_train_images = sorted(glob.glob(os.path.join(train_img_dir, '*.tif')))\nall_train_masks = sorted(glob.glob(os.path.join(train_mask_dir, '*.tif')))\ntrain_images, val_images, train_masks, val_masks = train_test_split(all_train_images, all_train_masks, test_size=0.2, random_state=42)\ntrain_transform = A.Compose([A.Resize(Config.IMG_HEIGHT, Config.IMG_WIDTH), A.Rotate(35), A.HorizontalFlip(), A.Normalize(0, 1), ToTensorV2()])\nval_transform = A.Compose([A.Resize(Config.IMG_HEIGHT, Config.IMG_WIDTH), A.Normalize(0, 1), ToTensorV2()])\ntrain_dataset = InriaDataset(train_images, train_masks, transform=train_transform)\nval_dataset = InriaDataset(val_images, val_masks, transform=val_transform)\ntrain_loader = DataLoader(train_dataset, Config.BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, Config.BATCH_SIZE, shuffle=False, num_workers=2)\n\n# --- Start the Optuna Study ---\n# This will run 20 different experiments to find the best settings.\nstudy = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=20)\n\n# --- Print the Best Results ---\nprint(\"\\n--- Optimization Complete ---\")\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value (Dice Score): \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# --- Train the Final Model with the BEST settings ---\nprint(\"\\n--- Training the FINAL model with the best hyperparameters ---\")\nbest_params = study.best_params\nfinal_model = ResNet_UNet().to(Config.DEVICE)\nfinal_optimizer = getattr(torch.optim, best_params[\"optimizer\"])(final_model.parameters(), lr=best_params[\"lr\"])\nfinal_loss_fn = DiceBCELoss()\n\nfor epoch in range(50): # Train for longer on the final run\n    print(f\"Final Training - Epoch {epoch+1}/50\")\n    train_fn(train_loader, final_model, final_optimizer, final_loss_fn)\n\n# Save the final, best model\ntorch.save(final_model.state_dict(), Config.MODEL_SAVE_PATH)\nprint(f\"\\n🏆 Final BEST model saved to {Config.MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T12:34:10.982805Z","iopub.execute_input":"2025-09-03T12:34:10.983095Z","execution_failed":"2025-09-03T15:44:16.042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}